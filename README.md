# Project Sophia: Make Neural Networks from Scratch

~~Plagiarized from~~ Inspired by [Andrej Karparthy's cs231n](http://cs231n.github.io/neural-networks-case-study/) and [Colah's Blog](http://colah.github.io/posts/2015-08-Backprop/).

This notebook recreates neural network algorithms using only the linear algebra library `numpy` (uses other libraries for performance benchmark and visualization). The sample data is a 2-dimensional data with three classes in a form of spirals. 

The content consists of the following aspects of neural networks:

* Weight initialization
* Loss functions
    * Cross-entropy loss
    * Hinge loss
    * Regularization loss
* Layers
    * Linear
    * Sigmoid
    * Tanh
    * ReLu
    * Swish
    * Softmax
    * SVM
    * Dropout (TBD)
    * Batch Normalization (TBD)
    * Convolution (TBD)
    * LSTM (TBD)
    * Embeddings (TBD)
* Training
    * Linear > Softmax
    * Linear > SVM
    * [Linear > Activation]++ > [Linear > Output]
* Optimization (TBD)
* Validation (TBD)
